{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA1x5l3QlWqC"
      },
      "source": [
        "# 1.3「転移学習」で少量データの分類を実現する方法\n",
        "\n",
        "- 本ファイルでは、学習済みのVGGモデルを使用し、転移学習でアリとハチの画像を分類するモデルを学習します\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRJbymzfozM5"
      },
      "source": [
        "## 本ファイルは、Google Colabでの実行を前提としています"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82UCc0cflWqI"
      },
      "source": [
        "# 学習目標\n",
        "\n",
        "1. 画像データからDatasetを作成できるようになる\n",
        "2. DataSetからDataLoaderを作成できるようになる\n",
        "3. 学習済みモデルの出力層を任意の形に変更できるようになる\n",
        "4. 出力層の結合パラメータのみを学習させ、転移学習が実装できるようになる\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJN48pwclWqJ"
      },
      "source": [
        "# 事前準備\n",
        "\n",
        "1. 書籍の指示に従い、本章で使用するデータをダウンロード\n",
        "\n",
        "2. forループの経過時間と残り時間を計測するパッケージtqdmをインストールします。\n",
        "\n",
        "conda install -c conda-forge tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWDnhY8Plo4P",
        "outputId": "9bf366b5-0c33-4f2f-a723-bcdc1f205d4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'internship' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rin-nishikawa/internship.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLva_hyUlxlf",
        "outputId": "1a9528d2-539a-4296-9edb-7793cdd0b464"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'ls' �́A�����R�}���h�܂��͊O���R�}���h�A\n",
            "����\\�ȃv���O�����܂��̓o�b�` �t�@�C���Ƃ��ĔF������Ă��܂���B\n"
          ]
        }
      ],
      "source": [
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dLafiH3l0v2",
        "outputId": "d3476745-7a8f-479f-caf3-42f1bc6a5377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\000166JP6\\Documents\\internship\\internship\\1_image_classification\\internship\n"
          ]
        }
      ],
      "source": [
        "%cd \"internship\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw4aia4Dl3F2",
        "outputId": "1df93c9f-938b-4e8d-9f75-cbf721b02e30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'ls' �́A�����R�}���h�܂��͊O���R�}���h�A\n",
            "����\\�ȃv���O�����܂��̓o�b�` �t�@�C���Ƃ��ĔF������Ă��܂���B\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6LE69pul4zW",
        "outputId": "27b8663a-ebb5-4f09-d477-2d71cdd1d372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\000166JP6\\Documents\\internship\\internship\\1_image_classification\\internship\\1_image_classification\n"
          ]
        }
      ],
      "source": [
        "%cd \"1_image_classification\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YIxC8nkmLgB",
        "outputId": "59315278-1a73-4e23-e654-2c2aeb6ec5a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'ls' �́A�����R�}���h�܂��͊O���R�}���h�A\n",
            "����\\�ȃv���O�����܂��̓o�b�` �t�@�C���Ƃ��ĔF������Ă��܂���B\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "teXfeIJxmMkw"
      },
      "outputs": [],
      "source": [
        "# make_folders_and_data_downloads.ipynbの中身を実行\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "\n",
        "data_dir = \"./data/\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir)\n",
        "\n",
        "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
        "save_path = os.path.join(data_dir, \"imagenet_class_index.json\")\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    urllib.request.urlretrieve(url, save_path)\n",
        "\n",
        "url = \"https://download.pytorch.org/tutorial/hymenoptera_data.zip\"\n",
        "save_path = os.path.join(data_dir, \"hymenoptera_data.zip\")\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    urllib.request.urlretrieve(url, save_path)\n",
        "\n",
        "    # ZIPファイルを読み込み\n",
        "    zip = zipfile.ZipFile(save_path)\n",
        "    zip.extractall(data_dir)  # ZIPを解凍\n",
        "    zip.close()  # ZIPファイルをクローズ\n",
        "\n",
        "    # ZIPファイルを消去\n",
        "    os.remove(save_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "V4M1yJuxmVyp"
      },
      "outputs": [],
      "source": [
        "# 追記以上--------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7Nk2_bfilWqJ"
      },
      "outputs": [],
      "source": [
        "# パッケージのimport\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EsQFy7nflWqK"
      },
      "outputs": [],
      "source": [
        "# 乱数のシードを設定\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0wuE1LQlWqK"
      },
      "source": [
        "# DataSetを作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6PerlS6ilWqK"
      },
      "outputs": [],
      "source": [
        "# 入力画像の前処理をするクラス\n",
        "# 訓練時と推論時で処理が異なる\n",
        "\n",
        "\n",
        "class ImageTransform():\n",
        "    \"\"\"\n",
        "    画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "    画像のサイズをリサイズし、色を標準化する。\n",
        "    訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    resize : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    mean : (R, G, B)\n",
        "        各色チャネルの平均値。\n",
        "    std : (R, G, B)\n",
        "        各色チャネルの標準偏差。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, resize, mean, std):\n",
        "        self.data_transform = {\n",
        "            'train': transforms.Compose([\n",
        "                transforms.RandomResizedCrop(\n",
        "                    resize, scale=(0.5, 1.0)),  # データオーギュメンテーション\n",
        "                transforms.RandomHorizontalFlip(),  # データオーギュメンテーション\n",
        "                transforms.ToTensor(),  # テンソルに変換\n",
        "                transforms.Normalize(mean, std)  # 標準化\n",
        "            ]),\n",
        "            'val': transforms.Compose([\n",
        "                transforms.Resize(resize),  # リサイズ\n",
        "                transforms.CenterCrop(resize),  # 画像中央をresize×resizeで切り取り\n",
        "                transforms.ToTensor(),  # テンソルに変換\n",
        "                transforms.Normalize(mean, std)  # 標準化\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, img, phase='train'):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        phase : 'train' or 'val'\n",
        "            前処理のモードを指定。\n",
        "        \"\"\"\n",
        "        return self.data_transform[phase](img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "mnzGcMN_lWqL",
        "outputId": "f7d4f271-7c1f-4028-a5fe-9c1d1721218d"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/1.jpeg'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\000166JP6\\Documents\\internship\\internship\\1_image_classification\\1_3_transfer_learning_on_GoogleColab.ipynb セル 17\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/000166JP6/Documents/internship/internship/1_image_classification/1_3_transfer_learning_on_GoogleColab.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 訓練時の画像前処理の動作を確認\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/000166JP6/Documents/internship/internship/1_image_classification/1_3_transfer_learning_on_GoogleColab.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# 実行するたびに処理結果の画像が変わる\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/000166JP6/Documents/internship/internship/1_image_classification/1_3_transfer_learning_on_GoogleColab.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/000166JP6/Documents/internship/internship/1_image_classification/1_3_transfer_learning_on_GoogleColab.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# 1. 画像読み込み\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/000166JP6/Documents/internship/internship/1_image_classification/1_3_transfer_learning_on_GoogleColab.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m image_file_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./data/1.jpeg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/000166JP6/Documents/internship/internship/1_image_classification/1_3_transfer_learning_on_GoogleColab.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(image_file_path)   \u001b[39m# [高さ][幅][色RGB]\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/000166JP6/Documents/internship/internship/1_image_classification/1_3_transfer_learning_on_GoogleColab.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# 2. 元の画像の表示\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/000166JP6/Documents/internship/internship/1_image_classification/1_3_transfer_learning_on_GoogleColab.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(img)\n",
            "File \u001b[1;32mc:\\Users\\000166JP6\\AppData\\Local\\miniconda3\\envs\\int\\Lib\\site-packages\\PIL\\Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3215\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3217\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3218\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   3219\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3221\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/1.jpeg'"
          ]
        }
      ],
      "source": [
        "# 訓練時の画像前処理の動作を確認\n",
        "# 実行するたびに処理結果の画像が変わる\n",
        "\n",
        "# 1. 画像読み込み\n",
        "image_file_path = './data/1.jpeg'\n",
        "img = Image.open(image_file_path)   # [高さ][幅][色RGB]\n",
        "\n",
        "# 2. 元の画像の表示\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "# 3. 画像の前処理と処理済み画像の表示\n",
        "size = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "transform = ImageTransform(size, mean, std)\n",
        "img_transformed = transform(img, phase=\"train\")  # torch.Size([3, 224, 224])\n",
        "\n",
        "# (色、高さ、幅)を (高さ、幅、色)に変換し、0-1に値を制限して表示\n",
        "img_transformed = img_transformed.numpy().transpose((1, 2, 0))\n",
        "img_transformed = np.clip(img_transformed, 0, 1)\n",
        "plt.imshow(img_transformed)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3gJ3XmJlWqM",
        "outputId": "23617ca5-dd0c-4bba-bfd7-e41fc270607e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./data/hymenoptera_data/train/**/*.jpg\n",
            "./data/hymenoptera_data/val/**/*.jpg\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['./data/hymenoptera_data/train\\\\ants\\\\0013035.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1030023514_aad5c608f9.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1095476100_3906d8afde.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1099452230_d1949d3250.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\116570827_e9c126745d.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1225872729_6f0856588f.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1262877379_64fcada201.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1269756697_0bce92cdab.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1286984635_5119e80de1.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\132478121_2a430adea2.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1360291657_dc248c5eea.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1368913450_e146e2fb6d.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1473187633_63ccaacea6.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\148715752_302c84f5a4.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1489674356_09d48dde0a.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\149244013_c529578289.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\150801003_3390b73135.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\150801171_cd86f17ed8.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\154124431_65460430f2.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\162603798_40b51f1654.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1660097129_384bf54490.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\167890289_dd5ba923f3.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1693954099_46d4c20605.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\175998972.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\178538489_bec7649292.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1804095607_0341701e1c.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1808777855_2a895621d7.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\188552436_605cc9b36b.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1917341202_d00a7f9af5.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\1924473702_daa9aacdbe.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\196057951_63bf063b92.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\196757565_326437f5fe.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\201558278_fe4caecc76.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\201790779_527f4c0168.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\2019439677_2db655d361.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\207947948_3ab29d7207.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\20935278_9190345f6b.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\224655713_3956f7d39a.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\2265824718_2c96f485da.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\2265825502_fff99cfd2d.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\226951206_d6bf946504.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\2278278459_6b99605e50.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\2288450226_a6e96e8fdf.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\2288481644_83ff7e4572.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\2292213964_ca51ce4bef.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\24335309_c5ea483bb8.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\245647475_9523dfd13e.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\255434217_1b2b3fe0a4.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\258217966_d9d90d18d3.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\275429470_b2d7d9290b.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\28847243_e79fe052cd.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\318052216_84dff3f98a.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\334167043_cbd1adaeb9.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\339670531_94b75ae47a.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\342438950_a3da61deab.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\36439863_0bec9f554f.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\374435068_7eee412ec4.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\382971067_0bfd33afe0.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\384191229_5779cf591b.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\386190770_672743c9a7.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\392382602_1b7bed32fa.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\403746349_71384f5b58.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\408393566_b5b694119b.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\424119020_6d57481dab.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\424873399_47658a91fb.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\450057712_771b3bfc91.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\45472593_bfd624f8dc.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\459694881_ac657d3187.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\460372577_f2f6a8c9fc.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\460874319_0a45ab4d05.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\466430434_4000737de9.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\470127037_513711fd21.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\474806473_ca6caab245.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\475961153_b8c13fd405.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\484293231_e53cfc0c89.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\49375974_e28ba6f17e.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\506249802_207cd979b4.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\506249836_717b73f540.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\512164029_c0a66b8498.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\512863248_43c8ce579b.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\518773929_734dbc5ff4.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\522163566_fec115ca66.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\522415432_2218f34bf8.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\531979952_bde12b3bc0.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\533848102_70a85ad6dd.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\535522953_308353a07c.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\540889389_48bb588b21.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\541630764_dbd285d63c.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\543417860_b14237f569.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\560966032_988f4d7bc4.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\5650366_e22b7e1065.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\6240329_72c01e663e.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\6240338_93729615ec.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\649026570_e58656104b.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\662541407_ff8db781e7.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\67270775_e9fdf77e9d.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\6743948_2b8c096dda.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\684133190_35b62c0c1d.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\69639610_95e0de17aa.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\707895295_009cf23188.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\7759525_1363d24e88.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\795000156_a9900a4a71.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\822537660_caf4ba5514.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\82852639_52b7f7f5e3.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\841049277_b28e58ad05.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\886401651_f878e888cd.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\892108839_f1aad4ca46.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\938946700_ca1c669085.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\957233405_25c1d1187b.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\9715481_b3cb4114ff.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\998118368_6ac1d91f81.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\ant photos.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\Ant_1.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\army-ants-red-picture.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\hormiga_co_por.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\kurokusa.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\MehdiabadiAnt2_600.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\Nepenthes_rafflesiana_ant.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\swiss-army-ant.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\termite-vs-ant.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\trap-jaw-ant-insect-bg.jpg',\n",
              " './data/hymenoptera_data/train\\\\ants\\\\VietnameseAntMimicSpider.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\1092977343_cb42b38d62.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\1093831624_fb5fbe2308.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\1097045929_1753d1c765.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\1232245714_f862fbe385.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\129236073_0985e91c7d.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\1295655112_7813f37d21.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\132511197_0b86ad0fff.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\132826773_dbbcb117b9.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\150013791_969d9a968b.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\1508176360_2972117c9d.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\154600396_53e1252e52.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\16838648_415acd9e3f.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\1691282715_0addfdf5e8.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\17209602_fe5a5a746f.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\174142798_e5ad6d76e0.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\1799726602_8580867f71.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\1807583459_4fe92b3133.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\196430254_46bd129ae7.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\196658222_3fffd79c67.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\198508668_97d818b6c4.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2031225713_50ed499635.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2037437624_2d7bce461f.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2053200300_8911ef438a.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\205835650_e6f2614bee.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\208702903_42fb4d9748.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\21399619_3e61e5bb6f.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2227611847_ec72d40403.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2321139806_d73d899e66.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2330918208_8074770c20.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2345177635_caf07159b3.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2358061370_9daabbd9ac.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2364597044_3c3e3fc391.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2384149906_2cd8b0b699.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2397446847_04ef3cd3e1.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2405441001_b06c36fa72.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2445215254_51698ff797.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2452236943_255bfd9e58.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2467959963_a7831e9ff0.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2470492904_837e97800d.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2477324698_3d4b1b1cab.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2477349551_e75c97cf4d.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2486729079_62df0920be.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2486746709_c43cec0e42.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2493379287_4100e1dacc.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2495722465_879acf9d85.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2528444139_fa728b0f5b.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2538361678_9da84b77e3.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2551813042_8a070aeb2b.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2580598377_a4caecdb54.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2601176055_8464e6aa71.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2610833167_79bf0bcae5.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2610838525_fe8e3cae47.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2617161745_fa3ebe85b4.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2625499656_e3415e374d.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2634617358_f32fd16bea.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2638074627_6b3ae746a0.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2645107662_b73a8595cc.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2651621464_a2fa8722eb.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2652877533_a564830cbf.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\266644509_d30bb16a1b.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2683605182_9d2a0c66cf.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2704348794_eb5d5178c2.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2707440199_cd170bd512.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2710368626_cb42882dc8.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2722592222_258d473e17.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2728759455_ce9bb8cd7a.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2756397428_1d82a08807.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2765347790_da6cf6cb40.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2781170484_5d61835d63.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\279113587_b4843db199.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2792000093_e8ae0718cf.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2801728106_833798c909.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2822388965_f6dca2a275.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2861002136_52c7c6f708.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2908916142_a7ac8b57a8.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\29494643_e3410f0d37.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2959730355_416a18c63c.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\2962405283_22718d9617.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\3006264892_30e9cced70.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\3030189811_01d095b793.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\3030772428_8578335616.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\3044402684_3853071a87.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\3074585407_9854eb3153.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\3079610310_ac2d0ae7bc.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\3090975720_71f12e6de4.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\3100226504_c0d4f1e3f1.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\342758693_c56b89b6b6.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\354167719_22dca13752.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\359928878_b3b418c728.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\365759866_b15700c59b.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\36900412_92b81831ad.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\39672681_1302d204d1.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\39747887_42df2855ee.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\421515404_e87569fd8b.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\444532809_9e931e2279.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\446296270_d9e8b93ecf.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\452462677_7be43af8ff.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\452462695_40a4e5b559.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\457457145_5f86eb7e9c.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\465133211_80e0c27f60.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\469333327_358ba8fe8a.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\472288710_2abee16fa0.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\473618094_8ffdcab215.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\476347960_52edd72b06.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\478701318_bbd5e557b8.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\507288830_f46e8d4cb2.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\509247772_2db2d01374.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\513545352_fd3e7c7c5d.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\522104315_5d3cb2758e.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\537309131_532bfa59ea.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\586041248_3032e277a9.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\760526046_547e8b381f.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\760568592_45a52c847f.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\774440991_63a4aa0cbe.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\85112639_6e860b0469.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\873076652_eb098dab2d.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\90179376_abc234e5f4.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\92663402_37f379e57a.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\95238259_98470c5b10.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\969455125_58c797ef17.jpg',\n",
              " './data/hymenoptera_data/train\\\\bees\\\\98391118_bdb1e80cce.jpg']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# アリとハチの画像へのファイルパスのリストを作成する\n",
        "\n",
        "\n",
        "def make_datapath_list(phase=\"train\"):\n",
        "    \"\"\"\n",
        "    データのパスを格納したリストを作成する。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    phase : 'train' or 'val'\n",
        "        訓練データか検証データかを指定する\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    path_list : list\n",
        "        データへのパスを格納したリスト\n",
        "    \"\"\"\n",
        "\n",
        "    rootpath = \"./data/hymenoptera_data/\"\n",
        "    target_path = osp.join(rootpath+phase+'/**/*.jpg')\n",
        "    print(target_path)\n",
        "\n",
        "    path_list = []  # ここに格納する\n",
        "\n",
        "    # globを利用してサブディレクトリまでファイルパスを取得する\n",
        "    for path in glob.glob(target_path):\n",
        "        path_list.append(path)\n",
        "\n",
        "    return path_list\n",
        "\n",
        "\n",
        "# 実行\n",
        "train_list = make_datapath_list(phase=\"train\")\n",
        "val_list = make_datapath_list(phase=\"val\")\n",
        "\n",
        "train_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADiGymTSlWqM",
        "outputId": "02e26af1-1b30-4634-a5fb-0bf4901fc8a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# アリとハチの画像のDatasetを作成する\n",
        "\n",
        "\n",
        "class HymenopteraDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    アリとハチの画像のDatasetクラス。PyTorchのDatasetクラスを継承。\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    file_list : リスト\n",
        "        画像のパスを格納したリスト\n",
        "    transform : object\n",
        "        前処理クラスのインスタンス\n",
        "    phase : 'train' or 'test'\n",
        "        学習か訓練かを設定する。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, file_list, transform=None, phase='train'):\n",
        "        self.file_list = file_list  # ファイルパスのリスト\n",
        "        self.transform = transform  # 前処理クラスのインスタンス\n",
        "        self.phase = phase  # train or valの指定\n",
        "\n",
        "    def __len__(self):\n",
        "        '''画像の枚数を返す'''\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        前処理をした画像のTensor形式のデータとラベルを取得\n",
        "        '''\n",
        "\n",
        "        # index番目の画像をロード\n",
        "        img_path = self.file_list[index]\n",
        "        img = Image.open(img_path)  # [高さ][幅][色RGB]\n",
        "\n",
        "        # 画像の前処理を実施\n",
        "        img_transformed = self.transform(\n",
        "            img, self.phase)  # torch.Size([3, 224, 224])\n",
        "\n",
        "        # 画像のラベルをファイル名から抜き出す\n",
        "        if self.phase == \"train\":\n",
        "            label = img_path[30:34]\n",
        "        elif self.phase == \"val\":\n",
        "            label = img_path[28:32]\n",
        "\n",
        "        # ラベルを数値に変更する\n",
        "        if label == \"ants\":\n",
        "            label = 0\n",
        "        elif label == \"bees\":\n",
        "            label = 1\n",
        "\n",
        "        return img_transformed, label\n",
        "\n",
        "\n",
        "# 実行\n",
        "train_dataset = HymenopteraDataset(\n",
        "    file_list=train_list, transform=ImageTransform(size, mean, std), phase='train')\n",
        "\n",
        "val_dataset = HymenopteraDataset(\n",
        "    file_list=val_list, transform=ImageTransform(size, mean, std), phase='val')\n",
        "\n",
        "# 動作確認\n",
        "index = 0\n",
        "print(train_dataset.__getitem__(index)[0].size())\n",
        "print(train_dataset.__getitem__(index)[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M2pUtcklWqN"
      },
      "source": [
        "# DataLoaderを作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TwF8UealWqN",
        "outputId": "8c0a5ed3-8a7d-407c-bb72-0cc4be80648f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 3, 224, 224])\n",
            "tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 1, 1, 0, 1, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "# ミニバッチのサイズを指定\n",
        "batch_size = 32\n",
        "\n",
        "# DataLoaderを作成\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 辞書型変数にまとめる\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
        "\n",
        "# 動作確認\n",
        "batch_iterator = iter(dataloaders_dict[\"train\"])  # イテレータに変換\n",
        "inputs, labels = next(\n",
        "    batch_iterator)  # 1番目の要素を取り出す\n",
        "print(inputs.size())\n",
        "print(labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDdKxowclWqN"
      },
      "source": [
        "# ネットワークモデルの作成する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "350e40ad89fd4b289557201d12bd1878",
            "44a3efe233ad48699e9f59747f120f8d",
            "241cd1502c7c473ca0f525562f268c92",
            "96edb32a99d04f7d8bb9f161a699d4d3",
            "1b891dddf11343e696e5fd695c5a71df",
            "bc11034b13794c8db6b5dc2f4543932a",
            "d6b27aef63334910b4ec0b78b666684d",
            "af72d2b92986469d8abb55ab7b2ca180"
          ]
        },
        "id": "AsfGZ9B7lWqO",
        "outputId": "d7c303e3-b1e8-4229-ab39-c209cb820410"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\000166JP6\\AppData\\Local\\miniconda3\\envs\\int\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\000166JP6\\AppData\\Local\\miniconda3\\envs\\int\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ネットワーク設定完了：学習済みの重みをロードし、訓練モードに設定しました\n"
          ]
        }
      ],
      "source": [
        "# 学習済みのVGG-16モデルをロード\n",
        "# VGG-16モデルのインスタンスを生成\n",
        "use_pretrained = True  # 未学習パラメータを使用\n",
        "net = models.vgg16(pretrained=use_pretrained)\n",
        "\n",
        "# VGG16の最後の出力層の出力ユニットをアリとハチの2つに付け替える\n",
        "net.classifier[6] = nn.Linear(in_features=4096, out_features=2)\n",
        "\n",
        "# 訓練モードに設定\n",
        "net.train()\n",
        "\n",
        "print('ネットワーク設定完了：学習済みの重みをロードし、訓練モードに設定しました')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8rap7ISlWqO"
      },
      "source": [
        "# 損失関数を定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wiJCU6VlWqO"
      },
      "outputs": [],
      "source": [
        "# 損失関数の設定\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt3ziuoUlWqO"
      },
      "source": [
        "# 最適化手法を設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxkNa9LvlWqP",
        "outputId": "a480e08e-674e-45c3-c95d-b8c2ff848608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classifier.6.weight\n",
            "classifier.6.bias\n",
            "-----------\n",
            "[Parameter containing:\n",
            "tensor([[-0.0133, -0.0051, -0.0067,  ...,  0.0068, -0.0040,  0.0070],\n",
            "        [ 0.0063, -0.0014,  0.0149,  ..., -0.0018, -0.0116, -0.0079]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-0.0020, -0.0065], requires_grad=True)]\n"
          ]
        }
      ],
      "source": [
        "# 転移学習で学習させるパラメータを、変数params_to_updateに格納する\n",
        "params_to_update = []\n",
        "\n",
        "# 学習させるパラメータ名\n",
        "update_param_names = [\"classifier.6.weight\", \"classifier.6.bias\"]\n",
        "\n",
        "# 学習させるパラメータ以外は勾配計算をなくし、変化しないように設定\n",
        "for name, param in net.named_parameters():\n",
        "    if name in update_param_names:\n",
        "        param.requires_grad = True\n",
        "        params_to_update.append(param)\n",
        "        print(name)\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# params_to_updateの中身を確認\n",
        "print(\"-----------\")\n",
        "print(params_to_update)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDfKFIDglWqP"
      },
      "outputs": [],
      "source": [
        "# 最適化手法の設定\n",
        "optimizer = optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ocQ2VbJlWqP"
      },
      "source": [
        "# 学習・検証を実施"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXgRLEXHlWqP"
      },
      "outputs": [],
      "source": [
        "# モデルを学習させる関数を作成\n",
        "\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-------------')\n",
        "\n",
        "        # epochごとの学習と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "\n",
        "            # 未学習時の検証性能を確かめるため、epoch=0の訓練は省略\n",
        "            if (epoch == 0) and (phase == 'train'):\n",
        "                continue\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for inputs, labels in tqdm(dataloaders_dict[phase]):\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = net(inputs)\n",
        "                    loss = criterion(outputs, labels)  # 損失を計算\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "                    \n",
        "  \n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    # イタレーション結果の計算\n",
        "                    # lossの合計を更新\n",
        "                    epoch_loss += loss.item() * inputs.size(0)  \n",
        "                    # 正解数の合計を更新\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率を表示\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0cuPPsxlWqQ",
        "outputId": "202ca950-75c7-4236-b7e1-b7cd3e55b688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "-------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:18<00:00,  3.72s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.7191 Acc: 0.4967\n",
            "Epoch 2/2\n",
            "-------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:30<00:00,  3.80s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.4691 Acc: 0.7325\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:18<00:00,  3.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val Loss: 0.1726 Acc: 0.9608\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 学習・検証を実行する\n",
        "num_epochs=2\n",
        "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NRfoFSZlWqQ"
      },
      "source": [
        "以上"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "net.eval()  # 推論モードに設定\n",
        "\n",
        "# モデルのネットワーク構成を出力\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ILSVRCのラベル情報をロードし辞書型変数を生成します\n",
        "ILSVRC_class_index = {'0': ['n01440764', 'ants'],'1': ['n01443537', 'bees']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 出力結果からラベルを予測する後処理クラス\n",
        "class ILSVRCPredictor():\n",
        "    \"\"\"\n",
        "    ILSVRCデータに対するモデルの出力からラベルを求める。\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    class_index : dictionary\n",
        "            クラスindexとラベル名を対応させた辞書型変数。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, class_index):\n",
        "        self.class_index = class_index\n",
        "\n",
        "    def predict_max(self, out):\n",
        "        \"\"\"\n",
        "        確率最大のILSVRCのラベル名を取得する。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        out : torch.Size([1, 1000])\n",
        "            Netからの出力。\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        predicted_label_name : str\n",
        "            最も予測確率が高いラベルの名前\n",
        "        \"\"\"\n",
        "        maxid = np.argmax(out.detach().numpy())\n",
        "        predicted_label_name = self.class_index[str(maxid)][1]\n",
        "\n",
        "        return predicted_label_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "入力画像の予測結果： bees\n"
          ]
        }
      ],
      "source": [
        "# ILSVRCPredictorのインスタンスを生成します\n",
        "predictor = ILSVRCPredictor(ILSVRC_class_index)\n",
        "\n",
        "# 入力画像を読み込む\n",
        "image_file_path = './data/1.jpeg'\n",
        "img = Image.open(image_file_path)  # [高さ][幅][色RGB]\n",
        "\n",
        "# 前処理の後、バッチサイズの次元を追加する\n",
        "resize = 224\n",
        "transform = ImageTransform(resize, mean, std)  # 前処理クラス作成\n",
        "img_transformed = transform(img)  # torch.Size([3, 224, 224])\n",
        "inputs = img_transformed.unsqueeze_(0)  # torch.Size([1, 3, 224, 224])\n",
        "\n",
        "# モデルに入力し、モデル出力をラベルに変換する\n",
        "out = net(inputs)  # torch.Size([1, 1000])\n",
        "result = predictor.predict_max(out)\n",
        "\n",
        "# 予測結果を出力する\n",
        "print(\"入力画像の予測結果：\", result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "1-3_transfer_learning_on_GoogleColab.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b891dddf11343e696e5fd695c5a71df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "241cd1502c7c473ca0f525562f268c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc11034b13794c8db6b5dc2f4543932a",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b891dddf11343e696e5fd695c5a71df",
            "value": 553433881
          }
        },
        "350e40ad89fd4b289557201d12bd1878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_241cd1502c7c473ca0f525562f268c92",
              "IPY_MODEL_96edb32a99d04f7d8bb9f161a699d4d3"
            ],
            "layout": "IPY_MODEL_44a3efe233ad48699e9f59747f120f8d"
          }
        },
        "44a3efe233ad48699e9f59747f120f8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96edb32a99d04f7d8bb9f161a699d4d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af72d2b92986469d8abb55ab7b2ca180",
            "placeholder": "​",
            "style": "IPY_MODEL_d6b27aef63334910b4ec0b78b666684d",
            "value": " 528M/528M [00:14&lt;00:00, 39.5MB/s]"
          }
        },
        "af72d2b92986469d8abb55ab7b2ca180": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc11034b13794c8db6b5dc2f4543932a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6b27aef63334910b4ec0b78b666684d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
